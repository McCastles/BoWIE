{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DataPreparation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMtZpwwIoT+LGdhLPN9pAV5"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ShpDv4SQ0Ae8","executionInfo":{"status":"ok","timestamp":1609329036203,"user_tz":-180,"elapsed":1059,"user":{"displayName":"Aleksei Haidukevich","photoUrl":"","userId":"05632149318093275495"}},"outputId":"91cff21b-db57-4872-859c-6cfb0c2981a4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9As5z9BW0Fsu"},"source":["# ! ls drive/MyDrive/BOWIE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E2XvhvYy8Xy2"},"source":["import gzip\n","import json\n","import numpy as np\n","import h5py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9teHUv80Fu6"},"source":["root = '/content/drive/MyDrive/BOWIE/'\n","subset_path = root + 'Data/Subset/'\n","aux_path = root + 'Data/Additional/'\n","weights_path = root + 'Weights/'\n","\n","# ==== Questions data ====\n","path_qdata_train = subset_path + 'qdata_train.gzip'\n","path_qdata_test = subset_path + 'qdata_test.gzip'\n","path_qdata_valid = subset_path + 'qdata_valid.gzip'\n","\n","\n","# ==== Annotations data ====\n","path_adata_train = subset_path + 'adata_train.gzip'\n","path_adata_test = subset_path + 'adata_test.gzip'\n","path_adata_valid = subset_path + 'adata_valid.gzip'\n","\n","\n","# ==== Additional data ====\n","\n","# VQA_image_features.h5 - contains the ResNet image features for all the images(train, val, test) as an array\n","PATH_TO_H5_FILE      = aux_path + 'VQA_image_features.h5'\n","\n","# VQA_img_features2id.json - contains the mapping from image_id to index in the .h5 file\n","PATH_TO_FEAT2ID_FILE = aux_path + 'VQA_img_features2id.json'\n","\n","# image_ids_vqa.json - contains all images IDs from our data subset\n","PATH_TO_IDS_FILE     = aux_path + 'image_ids_vqa.json'\n","\n","# imgid2imginfo.json - contains the flickr url (and more image information) for the MSCOCO training and validation dataset.\n","PATH_TO_ID2INFO_FILE = aux_path + 'imgid2imginfo.json'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlXm60HE0Fw8"},"source":["def read_image_data():\n","    \n","    # load image features from hdf5 file and convert it to numpy array\n","    img_features = np.asarray(h5py.File(PATH_TO_H5_FILE, 'r')['img_features'])\n","    # img_features = list(h5py.File(PATH_TO_H5_FILE, 'r')['img_features'])\n","\n","    # load IDs file\n","    with open(PATH_TO_IDS_FILE, 'r') as f:\n","        img_ids = json.load(f)['image_ids']\n","\n","    # load feature mapping file\n","    with open(PATH_TO_FEAT2ID_FILE, 'r') as f:\n","        visual_feat_mapping = json.load(f)['VQA_imgid2id']\n","\n","    # load info mapping file\n","    with open(PATH_TO_ID2INFO_FILE, 'r') as f:\n","        imgid2info = json.load(f)\n","\n","    return img_ids, img_features, visual_feat_mapping, imgid2info\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n1Npr7Zx0Fy_"},"source":["def read_textual_data():\n","    \n","    data = []\n","\n","    for i, filepath in enumerate([ path_qdata_train, path_qdata_test, path_qdata_valid ]):\n","        print('Reading', filepath, '...')\n","        with gzip.open(filepath, 'r') as fin:\n","            w = [[x['question'], x['image_id']] for x in json.loads(fin.read().decode('utf-8'))['questions']]\n","            # print(len(w))\n","            data.append( w )\n","\n","    for i, filepath in enumerate([ path_adata_train, path_adata_test, path_adata_valid ]):\n","        print('Reading', filepath, '...')\n","        with gzip.open(filepath, 'r') as fin:\n","            w = [[x['multiple_choice_answer'], x['image_id']] for x in json.loads(fin.read().decode('utf-8'))['annotations']]\n","            # print(len(w))\n","            data.append( w )\n","\n","\n","    # for w in data:\n","    #     print( w[0], '\\n')\n","    \n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mzN-6C_K0F1V"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uxeya08l0F3q"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J0IyHyJD0F6N"},"source":["# map img_id to list of visual features corresponding to that image\n","def img_id_to_feat(img_id, visual_feat_mapping, img_features):\n","    # print(len(visual_feat_mapping), len(img_features))\n","    h5_id = visual_feat_mapping[str(img_id)]\n","    return img_features[h5_id]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qdggv1BU0F80"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eqUAlrfK0GCV"},"source":["def train_valid_test_data(\n","    q_train, q_valid, q_test, a_train, a_valid, a_test,\n","    visual_feat_mapping, img_features,\n","    TRAIN_LEN, VALID_LEN, TEST_LEN\n","):\n","\n","    bag = { key:[] for key in [\n","        'train_data', 'train_visual',\n","        'valid_data', 'valid_visual',\n","        'test_data', 'test_visual'\n","    ]}\n","\n","    for i in range(TRAIN_LEN):\n","        \n","        question = q_train[i][0].split()\n","        answer = a_train[i]\n","        answer, img_id = answer[0], answer[1]\n","\n","        if visual_feat_mapping.get(str(img_id)) != None:\n","\n","            content = ( question, answer, img_id  )\n","            bag['train_data'].append( content )\n","            bag['train_visual'].append( img_id_to_feat( img_id, visual_feat_mapping, img_features ) )\n","\n","\n","    for i in range(VALID_LEN):\n","        \n","        question = q_valid[i][0].split()\n","        answer = a_valid[i]\n","        answer, img_id = answer[0], answer[1]\n","        if visual_feat_mapping.get(str(img_id)) != None:\n","            content = ( question, answer, img_id  )\n","            bag['valid_data'].append( content )\n","            bag['valid_visual'].append( img_id_to_feat( img_id, visual_feat_mapping, img_features ) )\n","\n","    for i in range(TEST_LEN):\n","        \n","        question = q_test[i][0].split()\n","        answer = a_test[i]\n","        answer, img_id = answer[0], answer[1]\n","        if visual_feat_mapping.get(str(img_id)) != None:\n","            content = ( question, answer, img_id  )\n","            bag['test_data'].append( content )\n","            bag['test_visual'].append( img_id_to_feat( img_id, visual_feat_mapping, img_features ) )\n","\n","\n","\n","\n","    for part, before in zip([\"train_data\", \"valid_data\", \"test_data\"], [TRAIN_LEN, VALID_LEN, TEST_LEN]):\n","        print( f'\\n{part}' )\n","        print( f'Before filtering: {before}' )\n","        after = len(bag[part])\n","        print( f'After filtering: {after}' )\n","        print( f'Filtered out: { round((1-after/before)*100, 1)  }\\n' )\n","\n","    return bag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kjT2vjU30GEH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RYofRWPC0pDG"},"source":["def clean(word):\n","    if word[-1]=='?':\n","        word = word[:-1]\n","    if '/' in word:\n","        word = word.split('/')[0]\n","    if '\\\\' in word:\n","        word = word.split('\\\\')[0]\n","    if \"'s\" in word:\n","        word = word[:-2]\n","    return word.lower()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EQD99AsE0qEL"},"source":["def vocabulary(data):\n","    questions_vocabulary = {}\n","    answers_vocabulary = {}\n","    \n","    for sentence, answer, img_id in data['train_data'] + data['valid_data'] + data['test_data']:\n","        # print(sentence, answer, img_id)\n","        for word in sentence:\n","            word = clean(word)\n","            if word not in questions_vocabulary:\n","                questions_vocabulary[word] = len(questions_vocabulary)\n","        if answer not in answers_vocabulary:\n","            answers_vocabulary[answer] = len(answers_vocabulary)\n","            \n","            \n","    return questions_vocabulary, answers_vocabulary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cr_WkES-0p7o"},"source":["def stats( prepared ):\n","\n","    pad = '=============='\n","    def title(text):\n","        print( f'\\n{pad} {text} {pad}\\n' )\n","\n","    print('\\nSTATS FOR NERDS')\n","\n","    title('DATA LENGTH')\n","    for k, v in prepared.get('data').items():\n","        print( f'{k}\\t{len(v)}\\t{len(v[0])}\\t{v[0]}\\n' )\n","\n","    title('QUESTIONS VOCABULARY')\n","    print( f'Number of unqiue words: {prepared.get(\"VOCAB_SIZE\")}' )\n","\n","    title('ANSWERS VOCABULARY')\n","    print( f'Total number of possible answers: {prepared.get(\"NUM_LABELS\")}' )\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uXr8nSz50p43"},"source":["def prepare_data():\n","\n","    # read in textual (question-answer) data\n","    q_train, q_valid, q_test, a_train, a_valid, a_test = read_textual_data()\n","\n","    # read in visual feature data\n","    img_ids, img_features, visual_feat_mapping, imgid2info = read_image_data()\n","\n","    # img_ids = filter_featureless( img_ids, visual_feat_mapping )\n","\n","    IMG_FEAT_SIZE = len(img_features[0])\n","    # print('Each feature is a', IMG_FEAT_SIZE, 'vector')\n","\n","    fraction = 1\n","    TRAIN_LEN = int(fraction * len(q_train))\n","    VALID_LEN = int(fraction * len(q_valid))\n","    TEST_LEN =  int(fraction * len(q_test))\n","    # print(TRAIN_LEN, VALID_LEN, TEST_LEN)\n","\n","    data = train_valid_test_data(\n","        q_train, q_valid, q_test, a_train, a_valid, a_test,\n","        visual_feat_mapping, img_features,\n","        TRAIN_LEN, VALID_LEN, TEST_LEN\n","    )\n","\n","    questions_vocabulary, answers_vocabulary = vocabulary( data )\n","    lookup = list(answers_vocabulary.keys())\n","\n","    VOCAB_SIZE = len(questions_vocabulary) # amount of unique words in questions\n","    NUM_LABELS = len(answers_vocabulary) # amount of unique words in answers\n","\n","    prepared = {\n","        'data': data,\n","        'questions_vocabulary': questions_vocabulary,\n","        'answers_vocabulary': answers_vocabulary,\n","        'lookup': lookup,\n","        'VOCAB_SIZE': VOCAB_SIZE,\n","        'NUM_LABELS': NUM_LABELS,\n","        'IMG_FEAT_SIZE': IMG_FEAT_SIZE,\n","        'TRAIN_LEN': len( data[\"train_data\"] ),\n","        'VALID_LEN': len( data[\"valid_data\"] ),\n","        'TEST_LEN': len( data[\"test_data\"] ),\n","        'imgid2info': imgid2info,\n","        'visual_feat_mapping': visual_feat_mapping,\n","        'img_features': img_features\n","    }\n","\n","    stats( prepared )\n","\n","    return prepared\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wY-kyEwdVcgx"},"source":["# data = read_textual_data()\n","# img_ids, img_features, visual_feat_mapping, imgid2info = read_image_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_L9A_Ns8QUD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1609329042414,"user_tz":-180,"elapsed":7183,"user":{"displayName":"Aleksei Haidukevich","photoUrl":"","userId":"05632149318093275495"}},"outputId":"0fd3d2e4-c51d-4ceb-95ad-6e4d83148e1d"},"source":["# p = prepare_data()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading /content/drive/MyDrive/BOWIE/Data/Subset/qdata_train.gzip ...\n","Reading /content/drive/MyDrive/BOWIE/Data/Subset/qdata_test.gzip ...\n","Reading /content/drive/MyDrive/BOWIE/Data/Subset/qdata_valid.gzip ...\n","Reading /content/drive/MyDrive/BOWIE/Data/Subset/adata_train.gzip ...\n","Reading /content/drive/MyDrive/BOWIE/Data/Subset/adata_test.gzip ...\n","Reading /content/drive/MyDrive/BOWIE/Data/Subset/adata_valid.gzip ...\n","\n","train_data\n","Before filtering: 136721\n","After filtering: 96882\n","Filtered out: 29.1\n","\n","\n","valid_data\n","Before filtering: 8430\n","After filtering: 6008\n","Filtered out: 28.7\n","\n","\n","test_data\n","Before filtering: 25849\n","After filtering: 18217\n","Filtered out: 29.5\n","\n","\n","STATS FOR NERDS\n","\n","============== DATA LENGTH ==============\n","\n","train_data\t96882\t3\t(['What', 'English', 'meal', 'is', 'this', 'likely', 'for?'], 'tea', 228478)\n","\n","train_visual\t96882\t2048\t[0.21652411 0.4433445  1.0575547  ... 0.43506446 0.16850366 0.24101193]\n","\n","valid_data\t6008\t3\t(['Is', 'there', 'a', 'bell', 'on', 'the', 'train?'], 'yes', 540769)\n","\n","valid_visual\t6008\t2048\t[0.02937265 0.24076903 0.4964406  ... 0.25023538 0.02920359 0.28305528]\n","\n","test_data\t18217\t3\t(['What', 'insurance', 'company', 'is', 'a', 'sponsor?'], 'state farm', 413918)\n","\n","test_visual\t18217\t2048\t[0.5279811  0.15348202 0.6348457  ... 0.61716455 0.40149727 0.24504988]\n","\n","\n","============== QUESTIONS VOCABULARY ==============\n","\n","Number of unqiue words: 9599\n","\n","============== ANSWERS VOCABULARY ==============\n","\n","Total number of possible answers: 8666\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SYUULNi3--az"},"source":["# %cd drive/MyDrive/BOWIE/Notebooks\n","# !jupyter nbconvert --to python DataPreparation.ipynb"],"execution_count":null,"outputs":[]}]}